{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongShortTermMemoryModel:\n",
    "    def __init__(self, encoding_size):\n",
    "        # Model constants\n",
    "        cell_state_size = 128\n",
    "\n",
    "        # Cells\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(cell_state_size)\n",
    "\n",
    "        # Model input\n",
    "        self.batch_size = tf.placeholder(tf.int32, [])  # Needed by cell.zero_state call, and can be dependent on usage (training or generation)\n",
    "        self.x = tf.placeholder(tf.float32, [None, None, encoding_size])  # Shape: [batch_size, max_time, encoding_size]\n",
    "        self.y = tf.placeholder(tf.float32, [None, None, encoding_size])  # Shape: [batch_size, max_time, encoding_size]\n",
    "        self.in_state = cell.zero_state(self.batch_size, tf.float32)  # Can be used as either an input or a way to get the zero state\n",
    "\n",
    "        # Model variables\n",
    "        W = tf.Variable(tf.random_normal([cell_state_size, encoding_size]))\n",
    "        b = tf.Variable(tf.random_normal([encoding_size]))\n",
    "\n",
    "        # Model operations\n",
    "        lstm, self.out_state = tf.nn.dynamic_rnn(cell, self.x, initial_state=self.in_state)  # lstm has shape: [batch_size, max_time, cell_state_size]\n",
    "\n",
    "        # Logits, where tf.einsum multiplies a batch of txs matrices (lstm) with W\n",
    "        logits = tf.nn.bias_add(tf.einsum('bts,se->bte', lstm, W), b)  # b: batch, t: time, s: state, e: encoding\n",
    "\n",
    "        # Predictor\n",
    "        self.f = tf.nn.softmax(logits)\n",
    "\n",
    "        # Cross Entropy loss\n",
    "        self.loss = tf.losses.softmax_cross_entropy(self.y, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_encodings = [\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0],  # ' '\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0],  # 'h'\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],  # 'e'\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],  # 'l'\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0],  # 'o'\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],  # 'w'\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0],  # 'r'\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1],  # 'd'\n",
    "]\n",
    "encoding_size = np.shape(char_encodings)[1]\n",
    "\n",
    "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
    "\n",
    "x_train = [char_encodings[0], char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7]]  # ' hello'\n",
    "y_train = [char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]]  # 'hello '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 13:06:29.448534 139808889096000 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0917 13:06:29.449350 139808889096000 deprecation.py:323] From <ipython-input-2-b5a3ce92a02a>:7: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0917 13:06:29.478246 139808889096000 deprecation.py:323] From <ipython-input-2-b5a3ce92a02a>:20: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0917 13:06:29.525546 139808889096000 deprecation.py:506] From /home/sindrtho/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0917 13:06:29.533948 139808889096000 deprecation.py:506] From /home/sindrtho/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0917 13:06:29.912673 139808889096000 deprecation.py:323] From /home/sindrtho/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 13:06:30.109953 139808889096000 deprecation.py:506] From /home/sindrtho/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = LongShortTermMemoryModel(encoding_size)\n",
    "\n",
    "# Training: adjust the model so that its loss is minimized\n",
    "minimize_operation = tf.train.RMSPropOptimizer(0.05).minimize(model.loss)\n",
    "\n",
    "# Create session object for running TensorFlow operations\n",
    "session = tf.Session()\n",
    "\n",
    "# Initialize tf.Variable objects\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Initialize model.in_state\n",
    "zero_state = session.run(model.in_state, {model.batch_size: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9\n",
      "loss 1.5750618\n",
      " hllo lo   l   wl   l    l     l     l     l      l  \n",
      "epoch 19\n",
      "loss 1.1620361\n",
      " hello wo r wd   wd  wd   wd  wd  wd   wd  wd  wd  wd\n",
      "epoch 29\n",
      "loss 0.97323537\n",
      " helo wor d  helo  wor d  helo  wor d  helo  wor d  h\n",
      "epoch 39\n",
      "loss 0.56165713\n",
      " helo world d wrd d held word d held word d held word\n",
      "epoch 49\n",
      "loss 0.17030548\n",
      " hello world hello world hello world hello world hell\n",
      "epoch 59\n",
      "loss 0.06155604\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 69\n",
      "loss 0.028967798\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 79\n",
      "loss 0.014044245\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 89\n",
      "loss 0.0069261305\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 99\n",
      "loss 0.0034620382\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 109\n",
      "loss 0.0017523557\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 119\n",
      "loss 0.0008979179\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 129\n",
      "loss 0.00046551204\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 139\n",
      "loss 0.00024395196\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 149\n",
      "loss 0.00012917157\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 159\n",
      "loss 6.8988935e-05\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 169\n",
      "loss 3.7182315e-05\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 179\n",
      "loss 2.0235451e-05\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 189\n",
      "loss 1.1086362e-05\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 199\n",
      "loss 6.1988503e-06\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 209\n",
      "loss 3.5862013e-06\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 219\n",
      "loss 2.175565e-06\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 229\n",
      "loss 1.4305097e-06\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 239\n",
      "loss 1.0132779e-06\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 249\n",
      "loss 7.6492574e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 259\n",
      "loss 6.159143e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 269\n",
      "loss 5.265074e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 279\n",
      "loss 4.2716647e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 289\n",
      "loss 3.9736415e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 299\n",
      "loss 3.377596e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 309\n",
      "loss 3.1789136e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 319\n",
      "loss 2.7815494e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 329\n",
      "loss 2.3841852e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 339\n",
      "loss 2.1855033e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 349\n",
      "loss 1.88748e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 359\n",
      "loss 1.7881389e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 369\n",
      "loss 1.490116e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 379\n",
      "loss 1.3907749e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 389\n",
      "loss 1.3907749e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 399\n",
      "loss 1.2914337e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 409\n",
      "loss 1.1920927e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 419\n",
      "loss 1.490116e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 429\n",
      "loss 1.490116e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 439\n",
      "loss 1.1920927e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 449\n",
      "loss 1.0927517e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 459\n",
      "loss 1.1920927e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 469\n",
      "loss 1.1920928e-07\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 479\n",
      "loss 9.9341065e-08\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 489\n",
      "loss 9.9341065e-08\n",
      " hello world  hello world  hello world  hello world  \n",
      "epoch 499\n",
      "loss 1.0927517e-07\n",
      " hello world  hello world  hello world  hello world  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    session.run(minimize_operation, {model.batch_size: 1, model.x: [x_train], model.y: [y_train], model.in_state: zero_state})\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(\"epoch\", epoch)\n",
    "        print(\"loss\", session.run(model.loss, {model.batch_size: 1, model.x: [x_train], model.y: [y_train], model.in_state: zero_state}))\n",
    "\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        state = session.run(model.in_state, {model.batch_size: 1})\n",
    "        text = ' h'\n",
    "        y, state = session.run([model.f, model.out_state], {model.batch_size: 1, model.x: [[char_encodings[0]]], model.in_state: state})  # ' '\n",
    "        y, state = session.run([model.f, model.out_state], {model.batch_size: 1, model.x: [[char_encodings[1]]], model.in_state: state})  # 'h'\n",
    "        text += index_to_char[y.argmax()]\n",
    "        for c in range(50):\n",
    "            y, state = session.run([model.f, model.out_state], {model.batch_size: 1, model.x: [[char_encodings[y.argmax()]]], model.in_state: state})\n",
    "            text += index_to_char[y[0].argmax()]\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
